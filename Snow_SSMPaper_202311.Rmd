---
title: "John Snow Project - Social Science & Medicine paper 'Re-Evaluating John Snow's 1856 South London Study'"
author: "[Thomas Coleman](http://www.hilerun.org/econ)"
output: html_notebook
---
# Examining Snow's 1856 Table VI - Data and Analysis for "Re-Evaluating John Snow's 1856 South London Study"

### See "Causality in the Time of Cholera" working paper at https://papers.ssrn.com/abstract=3262234 and my [John Snow project website](http://www.hilerun.org/econ/papers/snow)

### This notebook is licensed under the [BSD 2-Clause License](https://opensource.org/licenses/BSD-2-Clause)

## Introduction

This notebook has the data and analysis for the paper submitted to Social Science and Medicine "Re-Evaluating John Snow's 1856 South London Study"

+ Read in the data for various of Snow's tables, and Koch & Denike's tables. These are all csv files that I have transcribed
+ Calculate Snow's predicted counts and rates by subdistrict
+ Calculate Koch & Denike's t-test
+ Calculate example table to explain t-test versus sum of squares
+ Adjust population data from Simon for the problems outlined in the paper
  + Some subdistricts are reported to have Lambeth customers when they do not
  + Subdistrict population-by-supplier does not add to district population-by-supplier
  + Subdistrict population for S&V + Lambeth sometimes exceeds 1851 census and 1854 Simon estimates
+ Run regressions
  + OLS
  + Single Negative Binomial 


#### Background
With population data by sub-district and by water supply company (originally published in Simon 1856, "Report on the last two cholera-epidemics of London"), Snow was able to compare mortality at the sub-district level more closely than he could in his 1855 "On the mode of communication ...". In Table VI Snow used estimates of mortality rates for Southwark & Vauxhall versus Lambeth, combined with population fractions by sub-district, to calculate population-weighted predicted mortality by sub-district. Snow's goal was to show that differences in water supply was the predominant factor - more important than crowding or other factors - in accounting for differences across sub-districts. 

Snow, however, did not have the statistical tools and methodology available to us today, and his argument had neither the clarity nor the rigor we would demand today. This notebook first examines Snow's Table VI (as published) and discusses the error structure across sub-districts in some detail. This helps to explain why a simple approach (such as a paired *t*-test) is not appropriate for these count data. 


For a brief introduction to Snow's work, see:

+ **Snow's original 1855 monograph** (it is masterful): Snow, John. 1855. *On the Mode of Communication of Cholera*. 2nd ed. London: John Churchill. http://archive.org/details/b28985266.
+ **The best popular exposition I have found**: Johnson, Steven. 2007. *The Ghost Map: The Story of London’s Most Terrifying Epidemic--and How It Changed Science, Cities, and the Modern World*. Reprint edition. New York: Riverhead Books.
+ **Another good popular version**: Hempel, Sandra. 2007. *The Strange Case of the Broad Street Pump: John Snow and the Mystery of Cholera*. First edition. Berkeley: University of California Press.
+ **Tufte's classic discussion of Snow's mapping** (a topic I don't cover here): Tufte, Edward R. 1997. *Visual Explanations: Images and Quantities, Evidence and Narrative*. 1st edition. Graphics Press.
+ **Biography**: Vinten-Johansen, Peter, Howard Brody, Nigel Paneth, Stephen Rachman, and Michael Russell Rip. 2003. *Cholera, Chloroform and the Science of Medicine: A Life of John Snow*. Oxford; New York: Oxford University Press. Linked on-line resources https://johnsnow.matrix.msu.edu/snowworks.php




This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. The results are also saved in a self-contained html document with the suffix *.nb.html*. If you want pure r code (for example to run outside RStudio) you can easily extract code with the command *knit('notebook.Rmd',tangle=TRUE)* which will save a file 'notebook.R' under your working directory.

Try executing the chunk below by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r message=FALSE, results='hide'}
# Copyright (c) 2023, Thomas Coleman
#
#  -------  Licensed under BSD 2-Clause "Simplified" License  -------
#
# Results and discussion in "Causality in the Time of Cholera: John Snow as a Prototype 
# for Causal Inference (Working Paper)" available at SSRN: https://papers.ssrn.com/abstract=3262234

rm(list=ls())    # starts a fresh workspace
#
library(knitr)
options(scipen=5)
# The following libraries are used for the Negative Binomial regression and the robust standard error analysis
#install.packages("sandwich")
#install.packages("lmtest")
library("MASS")
library("sandwich") 
library("lmtest") 
library("lme4")           ## For random effects Poisson & Negative Binomial
library("dplyr")          ## load For doing data manipulation, such as group_by and sum. The "select" function gets 
                          # masked so call by dplyr::select

# Read in the data from John Snow 1856, "Cholera and the water supply in the south district of London in 1854", 
#   These data were copied from the 1936 book "Snow on cholera, being a reprint of two papers" edited by Frost
# Table V by District (for running Poisson & Neg Binomial count regressions)
# Table VI by sub-district (for running Koch & Denike's tests)

# Table I "Showing the results of the Author's personal Inquiry into Twenty-One Sub-Districts"
# Table II "Showing the results of Inquiry made by Mr. Whiting in Eleven Sub-Districts"
# (My "tablei_1856" combines Snow's Tables I & II)

#tablei_1856 <- read.csv(file="Snow1856_TableI.csv",
#  header=TRUE, sep=",", skip=5,comment.char="#")

tableV_1856 <- read.csv(file="Snow1856_TableV.csv",
  header=TRUE, sep=",", skip=5,comment.char="#")
tableVI_1856 <- read.csv(file="Snow1856_TableVI.csv",
  header=TRUE, sep=",", skip=5,comment.char="#")

KDFig7 <- read.csv(file="KochDenikeFigure7.csv",header=TRUE, sep=",", skip=4,comment.char="#")

```

## Check Snow's Calculations for Table VI

There are a few rounding errors in Snow's Table VI, for items such as "mortality_1854" (the calculated mortality rate per 10,000 for 1854). The input .csv file has Snow's original entries but we can also recalculate the calculated and predicted entries and compare. The result of the code chunk below is a dataframe "xcompare" that has the differences between Snow's entry and the (rounded) calculation. Most are zero. The only real error is the projected mortality rate for Southwark, Christchurch which Snow reports as 57 while it should be 51.0. 

For references, the calculated entries are:

* "pop_combined" (estimated population for Southwark & Vauxhall and Lambeth customers combined) - no errors. Note that this population may be substantially less than the overall population, presumably because there are customers not served by a water company
* "mortality_1854" - mortality rate per 10,000, a few rounding errors
* "deaths_Southwark_projected_rate160" - number of deaths for Southwark customers projected using population and mortality rate of 160 per 10,000
* "deaths_Lambeth_projected_rate27" - number of deaths for Lambeth customers projected using population and mortality rate of 27 per 10,000
* "deaths_projected_combined" - Southwark and Lambeth deaths combined
* "mortality_projected" - the combined deaths divided by the combined population


```{r}
# Calculate data and print out differences between Snow's reported data and re-calculated fractions, etc.
# There are a number of minor differences, primarily rounding errors. 

tableVI_1856$pop_combined_calc = tableVI_1856$pop_southwark + tableVI_1856$pop_lambeth 
tableVI_1856$mortality_1854_calc = 10000 * tableVI_1856$deaths_1854 / tableVI_1856$pop1851
tableVI_1856$deaths_Southwark_projected_rate160_calc = 160 * tableVI_1856$pop_southwark / 10000
tableVI_1856$deaths_Lambeth_projected_rate27_calc = 27 * tableVI_1856$pop_lambeth / 10000
tableVI_1856$deaths_projected_combined_calc = tableVI_1856$deaths_Southwark_projected_rate160_calc +
	tableVI_1856$deaths_Lambeth_projected_rate27_calc
tableVI_1856$mortality_projected_calc = 10000 * tableVI_1856$deaths_projected_combined_calc /
	tableVI_1856$pop_combined_calc

xcompare <- tableVI_1856[c("subDistrict","pop_combined","mortality_1854","deaths_Southwark_projected_rate160",
	"deaths_Lambeth_projected_rate27","deaths_projected_combined","mortality_projected")]

xcompare$pop_combined <- round(tableVI_1856$pop_combined - tableVI_1856$pop_combined_calc,digits=0)
xcompare$mortality_1854 <- round(tableVI_1856$mortality_1854 - tableVI_1856$mortality_1854_calc,digits=0)
xcompare$deaths_Southwark_projected_rate160 <- round(tableVI_1856$deaths_Southwark_projected_rate160 - tableVI_1856$deaths_Southwark_projected_rate160_calc,digits=0)
xcompare$deaths_Lambeth_projected_rate27 <- round(tableVI_1856$deaths_Lambeth_projected_rate27 - tableVI_1856$deaths_Lambeth_projected_rate27_calc,digits=0)
xcompare$deaths_projected_combined <- round(tableVI_1856$deaths_projected_combined - tableVI_1856$deaths_projected_combined_calc,digits=0)
xcompare$mortality_projected <- round(tableVI_1856$mortality_projected - tableVI_1856$mortality_projected_calc,digits=0)

```

#### Snow's Table VI with original and re-calculated entries

```{r}
tableVI_1856
```

#### Comparison of Snow's entries versus re-calculated

```{r}
xcompare
```

## Snow's Comparison of Actual vs Predicted Mortality
### An (Inappropriate) Paired *t*-test

Snow's goal with Table VI was to show that the difference in mortality was driven primarily by the difference in mortality for Southwark & Vauxhall customers versus Lambeth customers. He had the actual mortality by sub-district ("mortality_1854" in the .csv). He calculated a population-weighted mortality rate for each sub-district by calculating the counts for both suppliers and each sub-district and combining them. This is labeled "mortality_projected" (for Snow's original entry) and "mortality_projectd_calc" for the number calculated and reported to more decimals. 

Snow's argument is that these two, actual and predicted mortality, were close: 

>"it will be observed that the calculated mortality bears a very close relation to the real mortality in each subdistrict. ... and proves the overwhelming influence which the nature of the water supply exerted over the mortality, overbearing every other circumstance which could be expected to affect the progress of the epidemic. ... [This] probably supplies a greater amount of statistical evidence than was ever brought to bear on a medical subject."

```{r}
tableVI_1856[c("subDistrict","seq_1855","mortality_1854","mortality_projected","mortality_projected_calc")]
```

We need a more formal method for comparing the actual versus predicted. As a first (but incorrect) approach let us try a paired *t*-test: for each sub-district calculate the difference between actual and predicted and test whether the average across all sub-districts is different from zero. We need to do this for mortality *rates* rather than counts because the populations can be quite different: Snow is comparing the overall sub-district mortality against a prediction based on only Southwark and Lambeth supplied customers, a population that may be lower. 

For example for Putney the overall population is 5,280 while the estimated combined Southwark plus Lambeth population is only 74: many houses were supplied by pump-wells or the Thames. For Kennington 1st the total population is 24,261 while the combined Southwark and Lambeth population is only 18,483. The total observed count (total population 24,261) is 305, giving an estimated mortality rate of 125.7 per 10,000. If we apply the rate of 125.7 to the Southwark and Lambeth population (18,483) we would have an expected count of only 232.2. Comparing 305 versus 232.2 is a difference of 72.7 but this reflects only differing population size and not underlying mortality or infection; the rates are identical.

If we go ahead and perform a paired *t*-test on the differences in rates (even though, as we will see, it is not appropriate), we find that the differences are not significant. (It would be nice to compare differences in _log_ rates (ratio or percentage difference) but Dulwich has zero actual deaths so that does not work. For a count regression, of course, that is not a problem.)

```{r}
trates_snow <- t.test(tableVI_1856$mortality_1854_calc[1:31],tableVI_1856$mortality_projected_calc[1:31],paired=TRUE)
print(trates_snow)

```

The *t*-ratio for the paired differences is `r round(trates_snow$statistic,2)`, which is not significant. The reason is that the mean (`r round(mean(tableVI_1856$mortality_1854_calc[1:31] - tableVI_1856$mortality_projected_calc[1:31]),2)`) is relatively small compared to the standard deviation for the paired differences (`r round(sd(tableVI_1856$mortality_1854_calc[1:31] - tableVI_1856$mortality_projected_calc[1:31]),2)`). (Remember that the *t*-ratio is the ratio of the mean to the *standard error*, or the standard deviation divided by square-root number of observations, $\sqrt{`r 1+trates_snow$parameter`}$.)



### Running t-test with silly parameters

The prediction equation is 

$$
R_{i}^{pred}=w_{i,S\&V}\cdot R_{S\&V}+w_{i,Lam}\cdot R_{Lam}
$$
Snow chose $R_{S&V}=160$ and $R_{Lam}=27$ based on the region-level mortality rates, but we can pick alternate parameters to demonstrate how and why the t-test is not appropriate. In this case, we will choose $R_{S&V} = 40$ and $R_{Lam} = 187.9057$. These are arbitrary except that they have been chosen so that 1) mortality for S&V is much lower than Lambeth (contrary to the evidence) and 2) the average actual-versus-predicted for mortality rates is zero, and thus the t-ratio is zero. 



```{r}
#
# Calculate predicted mortality counts and rates based on these parameters
tableVI_1856$deaths_Southwark_projected_rate_alt = 40 * tableVI_1856$pop_southwark / 10000
#tableVI_1856$deaths_Southwark_projected_rate_alt = 160 * tableVI_1856$pop_southwark / 10000
tableVI_1856$deaths_Lambeth_projected_rate_alt = 187.9057 * tableVI_1856$pop_lambeth / 10000
#tableVI_1856$deaths_Lambeth_projected_rate_alt = 147 * tableVI_1856$pop_lambeth / 10000
tableVI_1856$deaths_projected_combined_alt = tableVI_1856$deaths_Southwark_projected_rate_alt +
	tableVI_1856$deaths_Lambeth_projected_rate_alt
tableVI_1856$mortality_projected_alt = 10000 * tableVI_1856$deaths_projected_combined_alt /
	tableVI_1856$pop_combined_calc

```


```{r}
print(trates_snow)
xmean <- mean(tableVI_1856$mortality_1854_calc[1:31])  # this is using the calculated mean
xavgss <- sum((tableVI_1856$mortality_1854_calc[1:31]-xmean)^2) / 31
xavgssresid_snow <- sum((tableVI_1856$mortality_1854_calc[1:31]-tableVI_1856$mortality_projected_calc[1:31])^2) / 31
fractVar_snow <- xavgssresid_snow / xavgss
print(paste("Snow's prediction: Avg Resid SS from mean, from prediction, and Fraction of variance explained:",round(xavgss,2),
            round(xavgssresid_snow,2), round(fractVar_snow,4), 1-round(fractVar_snow,4)))
trates_alt <- t.test(tableVI_1856$mortality_1854_calc[1:31],tableVI_1856$mortality_projected_alt[1:31],paired=TRUE)
print(trates_alt)
xavgssresid_alt <- sum((tableVI_1856$mortality_1854_calc[1:31]-tableVI_1856$mortality_projected_alt[1:31])^2) / 31
fractVar_alt <- xavgssresid_alt / xavgss
print(paste("Silly rates: Avg Resid SS from mean, from prediction, and Fraction of variance explained:",round(xavgss,2),
            round(xavgssresid_alt,2), round(fractVar_alt,4)))

```

Create table used in paper comparing St. Mary Magdalen and Lambeth Church (1st)

```{r}

paired_example <- array(0,dim=c(8,6))
colnames(paired_example) <- c("Actual","Snow Pred","Diff Act-Pred","Diff Act-Mean","Flawed Pred","Diff Act-Flaw")
rownames(paired_example) <- c("St. Mary Magdalen","Lambeth Church (1st)","Average (these 2)","Average (all 31)","Overall t-ratio","Average sum-of-squares (all 31)","Resid Variance / Mean Var","Fraction of Mean Var Explained")

xind <- c(6,16) # indexes of St. Mary Magdalen and Lambeth Church (1s)
paired_example[c("St. Mary Magdalen","Lambeth Church (1st)"),"Actual"] <- tableVI_1856$mortality_1854_calc[xind]
paired_example[c("St. Mary Magdalen","Lambeth Church (1st)"),"Snow Pred"] <- tableVI_1856$mortality_projected_calc[xind]
paired_example[c("St. Mary Magdalen","Lambeth Church (1st)"),"Diff Act-Pred"] <- tableVI_1856$mortality_1854_calc[xind] - tableVI_1856$mortality_projected_calc[xind]
paired_example["Average (these 2)","Diff Act-Pred"] <- mean(tableVI_1856$mortality_1854_calc[xind] - tableVI_1856$mortality_projected_calc[xind])
paired_example["Average (all 31)","Diff Act-Pred"] <- mean(tableVI_1856$mortality_1854_calc[1:31] - tableVI_1856$mortality_projected_calc[1:31])
paired_example["Overall t-ratio","Diff Act-Pred"] <- trates_snow$statistic
paired_example["Average sum-of-squares (all 31)","Diff Act-Pred"] <- xavgssresid_snow

paired_example[c("St. Mary Magdalen","Lambeth Church (1st)"),"Diff Act-Mean"] <- tableVI_1856$mortality_1854_calc[xind] - xmean
paired_example["Average sum-of-squares (all 31)","Diff Act-Mean"] <- xavgss

paired_example[c("St. Mary Magdalen","Lambeth Church (1st)"),"Flawed Pred"] <- tableVI_1856$mortality_projected_alt[xind]
paired_example[c("St. Mary Magdalen","Lambeth Church (1st)"),"Diff Act-Flaw"] <- tableVI_1856$mortality_1854_calc[xind] - tableVI_1856$mortality_projected_alt[xind]
paired_example["Average (these 2)","Diff Act-Flaw"] <- mean(tableVI_1856$mortality_1854_calc[xind] - tableVI_1856$mortality_projected_alt[xind])
paired_example["Average (all 31)","Diff Act-Flaw"] <- mean(tableVI_1856$mortality_1854_calc[1:31] - tableVI_1856$mortality_projected_alt[1:31])
paired_example["Overall t-ratio","Diff Act-Flaw"] <- trates_alt$statistic
paired_example["Average sum-of-squares (all 31)","Diff Act-Flaw"] <- xavgssresid_alt
paired_example[c("Resid Variance / Mean Var","Fraction of Mean Var Explained"),"Diff Act-Pred"] <- 100*c(fractVar_snow, 1-fractVar_snow)
paired_example[c("Resid Variance / Mean Var","Fraction of Mean Var Explained"),"Diff Act-Flaw"] <- 100*c(fractVar_alt, 1-fractVar_alt)

kable(paired_example,digits=1)

```





## Table V 
### Assigning the 623 "Unascertained" Deaths

I now turn from Table VI (data by sub-district but at the combined Southwark-plus-Lambeth level) to Table V (data by District but dis-aggregated by supplier: Southwark & Vauxhall, Lambeth, and "Other"). 

For the quasi-randomized comparisons in a later notebook we need deaths assigned to water source or supplier. There are three sources: Southwark & Vauxhall Company; Lambeth Company; and "Other" (pump-wells, the Thames, ditches). The tables that provide data by supplier are as follows:

+ Snow 1855 Table VII and VIII, Snow 1856 Tables I, II: by sub-district (32 sub-districts) for part of the epidemic: the four weeks ending 5th August and seven seeks ending 26th August. Collected by Snow and Whiting
+ Snow 1856 Table V: by Registration District (11 Districts) for the full 1854 epidemic (ending October) and matching the dates for Snow 1855 Table XII

The background to these data are that in the summer of 1854 during the London epidemic, Snow recognized the importance of measuring deaths by supplier. He spent his free time visiting all the houses with a recorded death and determining the source of water supply (see Snow's narrative, Snow 1855 p 76 ff - Snow is indeed the father of "shoe leather epidemiology"). Snow - with assistance from Mr. John Joseph Whiting, L.A.C. - ascertained the supply for deaths for the seven weeks ending 26th August 1854. Their results are reported in Snow 1855 Tables VII and VIII and Snow 1856 Tables I, II, and III. Snow also persuaded the Registrar-General to collect data for the rest of the epidemic. These data are reported in Snow 1856 Table IV, and the combined data (for the whole 1854 epidemic) in Table V. (Note - to my knowledge data assigning deaths to water supplier by sub-district were never published for the full 1854 epidemic, at least in a form that matched Snow's Table XII.)

There were, however, difficulties in determining the source for houses that were supplied by either the Southwark & Vauxhall Company or the Lambeth Company. Homeowners and particularly renters might not remember which company actually supplied the water. (Note that there was little difficulty for houses supplied by "Other" - pump-wells, Thames, etc.) Snow was diligent in determining the source (and developed a chemical test that could determine in the absence of definitive billing or other records). For the seven weeks ending 26th August there were 22 out of 1,514 deaths (1.5%) "unascertained". The Registrar-General's reporters were less careful - 601 out of 3,564 deaths (16.9%) unascertained. Note, however, that the *location* of each of these 623 deaths was known and assigned to District and sub-district. 

For the quasi-randomized comparisons discussed in a later notebook we would like to estimate or approximate the assignment of these 623 deaths to the two water companies. Snow himself proposed a simple but reasonable method:

>The instances in which the water supply was not specified, or not ascertained, in the returns made by the district registrars must evidently nearly all have been cases in which the house was supplied by one or other of the water companies, for, if the persons received no such supply, and obtained water from a pump well, canal, or ditch, there could be no difficulty in knowing the fact. Moreover, as the two water companies are guided by precisely the same regulations, the difficulty in ascertaining the supply is exactly the same with regard to one as the other; I, therefore, concluded that I could not be wrong in dividing the non-ascertained cases between the two companies in the same proportion as those which were ascertained, and I have done so at the foot of table V (Snow (1856) p. 247)

The following code chunk adjust Southwark and Lambeth deaths and rates in Table V by assigning the "unascertained" on a District-by-District basis using within-district proportions, basically following Snow's proposal. Note, importantly, that within a District this does *not* change the total number of deaths but simply re-assigns the "unascertained" to Southwark & Vauxhall and Lambeth. 



```{r}
tableV_1856$mortality <- 10000 * tableV_1856$deaths_total / tableV_1856$pop1851

# Create table to hold Koch & Denike's numbers
tableV_Koch <- tableV_1856

# Adjust Southwark and Lambeth deaths by "unascertained" by within-district proportion of Southwark vs Lambeth
tableV_1856$deaths_southwark_adj <- tableV_1856$deaths_southwark + tableV_1856$deaths_unascertained * tableV_1856$deaths_southwark / (tableV_1856$deaths_southwark + tableV_1856$deaths_lambeth)

tableV_1856$deaths_lambeth_adj <- tableV_1856$deaths_lambeth + tableV_1856$deaths_unascertained * tableV_1856$deaths_lambeth / (tableV_1856$deaths_southwark + tableV_1856$deaths_lambeth)

# Display relevant columns
tableV_1856[c("district","pop1851","deaths_southwark","deaths_lambeth","deaths_pumps","deaths_unascertained","deaths_total","mortality","deaths_southwark_adj","deaths_lambeth_adj")]


```

### Problems with Koch & Denike (2006)

T. Koch and K. Denike ("Rethinking John Snow's South London study: A Bayesian evaluation and recalculation", *Social Science & Medicine* vol 63, no 1, July 2006) undertake a valuable exercise to examine Snow (1856) and Snow's analysis of the South London data, focused most particularly on the 623 unascertained deaths. They state as their goal the correction of Snow's analysis: “This paper describes a previously unacknowledged methodological and conceptual problem in Snow’s 1856 argument. We review the context of the South London study, identify the problem and then correct it with an empirical Bayes estimation (EBE) approach.”

Unfortunately they fail in their goal. They fail for two important reasons. First, the statistical test they examine (paired t-test shown in their Figure 5) is both inappropriate and not properly applied. Second, and much more seriously, they seem to misunderstand or misinterpret Snow's data and analysis and as a result they inappropriately alter the mortality data in Snow 1856 Table V - the data from the Registrar-General used by Snow and others. Although not ill-intentioned this altering of the original mortality data does invalidate their analysis and conclusions. 

First consider the paired *t*-test. As discussed above, this test is not appropriate because it does not incorporate the fact that deaths are counts and mortality rates have standard errors that depend on population sizes. However, if the test were to be used it would have to be applied to *rate* and not counts (again because of differences in populations sizes, this time between pairs). 

First we re-print the results of the paired *t*-test based on rates:

```{r}
print(trates_snow)
```

This test shows no significant difference (on average) between the pairs, which would support Snow's assertion of a "close relation" between predicted and actual. 

Koch & Denike, however, run the test using counts:


```{r}
tcounts_snow <- t.test(tableVI_1856$deaths_1854[1:31],tableVI_1856$deaths_projected_combined_calc[1:31],paired=TRUE)
print(tcounts_snow)
```

This test does show a high degree of significance, rejecting the hypothesis that the counts are the same. From this Koch & Denike conclude "his [Snow's] conclusion was less than convincing". (Koch & Denike p. 278) But for the reasons discussed (the paired *t*-test test is not an appropriate test, and should in any case be run with rates), Koch & Denike's conclusion is incorrect. 

The more serious problem with Koch & Denike's paper is that they alter underlying data in a way that is neither necessary nor justified. The problem Koch & Denike set out to address is the 623 deaths that were not assigned to water company supplier (Southwark & Vauxhall Company versus Lambeth Company). They state “there were 623 houses in which cholera occurred that could not be assigned reflexively to any single district nor to either of the two water supplier areas.” (p. 275) The first part of this statement is simply incorrect while the second part is slightly confusing. There were 623 deaths in houses where the house could not be assigned to water supplier, but all those death (and the houses in which they occurred) were clearly assigned, in the original reports from the Registrar-General, to Registration District and sub-district. The assignments to Registration District are clearly shown in Snow's Table V. There has never (to my knowledge) been any question about the reliability of the Registrar-General's assignment of deaths to District or Sub-District – in contrast to assignment to water supplier within sub-district. Throughout, Koch & Denike imply, incorrectly, that the problem with the 623 deaths is spatial location, when in fact it is assignment (within sub-district and District) to water *supply*. 

Koch & Denike re-allocate the 623 deaths across water supplier *and* Registration Districts. In doing so they move deaths across Districts, deaths that were reliably located – assigned to District by the Registrar-General. The re-assignment across Districts is neither necessary nor justified and corrupts the original data. The re-assignment introduces substantive errors in mortality rates for those districts with either below-average or above-average unassigned deaths. 

The following code chunk creates a copy of Table V and inserts Koch & Denike's re-assigned deaths for Southwark & Vauxhall and Lambeth from their Figure 6. The code then calculates for each District the overall mortality (for all sources) according to Koch & Denike, and displays this next to the true mortality rate per 10,000 persons. 




```{r}
# Populate Koch & Denike's 
tableV_Koch$deaths_unascertained <- 0
tableV_Koch$deaths_southwark[1:10] <- c(467.96,317.63,943.92,447.88,527.36,605.61,307.59,405.02,237.06,6.78)
tableV_Koch$deaths_southwark[12] <- sum(tableV_Koch$deaths_southwark[1:10])
tableV_Koch$deaths_lambeth[1:10] <- c(82.37,1.28,1.32,112.82,66.72,157.72,9.03,38.24,1.26,2.43)
tableV_Koch$deaths_lambeth[12] <- sum(tableV_Koch$deaths_lambeth[1:10])
tableV_Koch$deaths_total <- tableV_Koch$deaths_southwark + tableV_Koch$deaths_lambeth + tableV_Koch$deaths_pumps + tableV_Koch$deaths_unascertained
tableV_Koch$mortality <- 10000 * tableV_Koch$deaths_total / tableV_Koch$pop1851
tableV_Koch$mortality_true <- tableV_1856$mortality

# Display some of the columns
tableV_Koch[c("district","pop1851","deaths_southwark","deaths_lambeth","deaths_pumps","deaths_unascertained","deaths_total","mortality","mortality_true")]

```

Comparing the actual mortality to Koch & Denike's altered mortality rates we can see that they are the same at the aggregate South London level (all Districts combined) but they differ substantially for a number of Districts. For example the mortality rate for St. Saviour, Southwark is increased to 156.8 from 137.4 (per 10,000), while Lambeth is reduced to 56.5 from. All of these changes in overall mortality at the Registration District level are arbitrary and counter to the observed data.

We can also run the paired t-test for counts and rates from their Figure 7. I cannot reproduce their claim that the t-test is 1.253 - for both counts and rates I get values similar to Snow's results: 3-something for counts, 1-something for rates:




```{r}
tcounts_KD <- t.test(KDFig7$deaths1854,KDFig7$deaths_bothKDpred,paired=TRUE)
trates_KD <- t.test(KDFig7$rate_KDpred,tableVI_1856$mortality_1854_calc[1:31],paired=TRUE)
print(tcounts_KD)
print(trates_KD)
```



## Regressions for Table VI


### Actual vs Predicted as Regression, 1854 only

In any case, we want to turn from Snow's prediction equation to a regression prediction equation:

$$
\ln\left(Rate_{subdist}\right)=\ln\left(count_{subdist}/population_{subdist}\right)=\hat{\rho}_{S}+\hat{\rho}_{L}\cdot F_{L}+\hat{\rho}_{O}\cdot F_{O}+\varepsilon_{s}
$$

with

+ $R_{subdis}$ -- Data – Observed mortality rate by subdistrict (for 1854)
+ $F_{S},\,\ldots$ -- Data – Observed fraction or proportion of subdistrict population supplied by Southwark, Lambeth, Other (calculated as \nicefrac{N_{Southwark}}{N_{Total}}, ...)
+ $\varepsilon$ -- Residual, difference between actual (R_{subdis}) and predicted
+ $\hat{\rho}_{S}$ -- Estimated coefficient \textendash{} (log) Southwark mortality rate, the base mortality rate (constant)
+ $\hat{\rho}_{L},\,\hat{\rho}_{O}$ -- Estimated coefficient \textendash{} (log) differential mortality rate relative to Southwark


First, create the data (including Lambeth-only subdistricts) and run linear regressions

```{r}

# Read in the data from Snow 1855 "On the mode of communication of cholera"
# The code for this is in a separate workbook, so that it can be used from multiple notebooks. 
# First, "knit" to convert to pure .R, then "source"

# Bug fix and extension 13-jun-2023 to flag Snow_ReadData to use either 1851 or other population
#   Based on variable (flag) "population_adj" which is set in the "calling file" (NB - this is not a robust method)
#     = 0: no adjustment to subdistrict, use original population
#     = 1: zero out Lambeth in Southwark-only subdistricts
#     = 2: adjust upwards so subdistrict sums match district reports
#     = 3: adjust by-supplier subdistrict downwards if combined SV + Lambeth is greater than 1854
#     = 4: adjust subdistrict population upwards if combined SV + Lambeth is greater than 1854. OVERWRITE POP1854!!! 
#  If not set, then use 3

#   Based on variable (flag) "population_1851" which is set in the "calling file" (NB - this is not a robust method)
#     = 0: use 1849 & 1854 population
#     = 1: use 1851 population (rather than 1849 & 1854)
#  If not set, then use 0

#population_1851 <- 1
knit('Snow_ReadDataFn.Rmd', tangle=TRUE)
source('Snow_ReadDataFn.R') 


# Read in tables (which can be used by functions in Snow_ReadDataFn)
# Read in the data from Snow 1855 "On the mode of communication of cholera"
tableviii <- read.csv(file="Snow1855_TableVIII.csv", header=TRUE, sep=",", skip=5,comment.char="#")
tablevii <- read.csv(file="Snow1855_TableVII.csv", header=TRUE, sep=",", skip=5,comment.char="#")
tablexii <- read.csv(file="Snow1855_TableXII.csv", header=TRUE, sep=",", skip=5,comment.char="#")

# Read in the data from John Snow 1856, "Cholera and the water supply in the south district of London in 1854", 
#   These data were copied from the 1936 book "Snow on cholera, being a reprint of two papers" edited by Frost
# Table V by District (for running Poisson & Neg Binomial count regressions)
# Table VI by sub-district (for running Koch & Denike's tests)

# Table I "Showing the results of the Author's personal Inquiry into Twenty-One Sub-Districts"
# Table II "Showing the results of Inquiry made by Mr. Whiting in Eleven Sub-Districts"
# (My "tableI_1856" combines Snow's Tables I & II)

tableI_1856 <- read.csv(file="Snow1856_TableI.csv", header=TRUE, sep=",", skip=5,comment.char="#")

tableV_1856 <- read.csv(file="Snow1856_TableV.csv", header=TRUE, sep=",", skip=5,comment.char="#")
tableVI_1856 <- read.csv(file="Snow1856_TableVI.csv", header=TRUE, sep=",", skip=5,comment.char="#")

tableSimonIII <- read.csv(file="Simon1856_TableIII_pop.csv",header=TRUE, sep=",", skip=5,comment.char="#")

tableSimonIII_StNoDeath <- read.csv(file="Simon1856_TableIII_StNoDeath.csv",header=TRUE, sep=",", skip=6,comment.char="#")

tableGROElevation <- read.csv(file="GRO1854Elevation.csv",header=TRUE, sep=",", skip=4,comment.char="#")



# Create "regdata" and "regdata54" based on adjusted subdistrict by-supplier estimates, 1854 population, and deaths (counts) from 1856 Table VI (rather than 1855 Table XII - slight differences in counts, I think updated in Snow's 1856)

# This will create "regdata" for all subdistricts, including Lambeth-only subdistricts
regdata <- cr_regdata(tableI_1856,tablevii,tableviii,tablexii,tableV_1856,tableVI_1856,tableSimonIII,tableSimonIII_StNoDeath,tableGROElevation,population_adj=3,population_1851=0,tableVIdeaths=1,excludeLambethOnly=0)

regdata$perc_otherThames <- regdata$perc_other * regdata$ThamesDummy  # Dummy because "other" for areas next to the Thames get water from Thames

regdata <- regdata[regdata$subDistrict != "Sydenham",] # Snow 1856 Table VI did not include Sydenham, so no by-supplier population
regdata54 <- regdata[regdata$year == 1854,]
regdata54$lnrate <- log(regdata54$rate/10000)
# De-mean housing density so that estimate housing density (pop_per_house) are all relative to mean
regdata54$pop_per_house <- regdata54$pop_per_house - mean(regdata54$pop_per_house)
regdata54$elevation <- regdata54$elevation - mean(regdata54$elevation)


```


```{r}
# Snow 1856 Table VI did not include Sydenham, but that will be excluded because the predicted rate will be nan

ols1854 <- lm(rate ~ perc_lambeth + perc_other + perc_otherThames, data=regdata54 )
summary(ols1854)

ols1854_popelev <- lm(rate ~ pop_per_house + elevation, data=regdata54 )
summary(ols1854_popelev)
ols1854_waterpopelev <- lm(rate ~ perc_lambeth + perc_other + perc_otherThames + pop_per_house + elevation, data=regdata54 )
summary(ols1854_waterpopelev)


```



Note a few things here:

+ The R-squared (`r round(summary(ols1854)$adj.r.squared,3)`) is higher here than above - which we should expect because we are now accounting for the "other"
+ Both Lambeth and "Other" show substantially reduced mortality relative to S&V (factors of `r round(exp(-summary(ols1854)$coefficients[c(2,3),1]),2)`)
+ Housing density is insignificant



But, we should run this as a count regression


I want to run a linear (rather than log) regressions but this is a little tricky to do with the glm and glm.nb functions. Those assume a log-link function, but I need to use the "identity" link. The problem is with the "offset" which is essentially (for the log link) setting:
$$\ln (count) = \alpha + \beta \cdot X + 1 \cdot \log (population) + \varepsilon $$
This is then equivalent to 
$$\ln (Rate) = \alpha + \beta \cdot X + \varepsilon $$
For the glm and glm.nb this is done with the "offset" function but that does not work with the identity link. The alternative is to run the regression
$$\ln (count) = \alpha \cdot population + \beta \cdot (X \cdot population) + \varepsilon $$
In other words to 

- multiply all the data by population
- run the regression with no constant but include population as a variable.

There is a problem in calculating things like the pseudo-$R^2$ and other $R^2$ statistics. The pseudo-$R^2$ is calculated as 
$$ 1 - \frac{ResidDeviance}{NullDeviance}$$
For the regular (non-identity-link) models the _ResidDeviance = model\$deviance_ and _NullDeviance = model\$null.deviance_. But for running the regression with no constant gives $NullDeviance = \infty$. Instead we can get the Null Deviance from the regular Poisson and Negative Binomial models. This means we need to modify the "overdisp_fn" function so that it works for linear (where there is no offset and the first element is population.)

```{r}
# For running linear count regressions, we need to weight all the regressors by the population (since we can't use the "offset()" method)
regdata_lin <- regdata 
# De-mean housing density so that estimate housing density (pop_per_house) are all relative to mean
regdata_lin$pop_per_house <- regdata_lin$pop_per_house - mean(regdata_lin$pop_per_house)
regdata_lin$elevation <- regdata_lin$elevation - mean(regdata_lin$elevation)
regdata_lin$population <- regdata_lin$population / 10000
regdata_lin$year1854 <- regdata_lin$year == 1854
xvars <- c("perc_other","perc_lambeth","pop_per_house","perc_otherThames","elevation","year1854","perc_lambeth54")
regdata_lin[,xvars] <-
	regdata_lin[,xvars] * regdata_lin$population


regdata54_lin <- regdata_lin[regdata_lin$year == 1854,]




```







And now Negative Binomial


```{r}

# Negative Binomial log version (needed for null deviance)
nb54 <- glm.nb(deaths ~ perc_lambeth + perc_other + perc_otherThames 
	 + offset(log(population)), data=regdata54)


# Negative Binomial with linear (identity) link function
nb54lin <- glm.nb(deaths ~ population + perc_lambeth + perc_other + perc_otherThames 
	 -1, link = identity, data=regdata54_lin)
nb54linrobustse <- coeftest(nb54lin, vcov = vcovHC(nb54lin))
summary(nb54lin)
#logLik(nb54lin)
overdisp_fun(nb54lin,nulldeviance = nb54$null.deviance)
#show(nb54linrobustse)


# Negative Binomial with water + housing density + elevation (elevation only does not converge nicely)
nb54denelev <- glm.nb(deaths ~ population + perc_lambeth + perc_other + perc_otherThames + pop_per_house + elevation
	 -1 , link = identity, data=regdata54_lin)
nb54denelevrobustse <- coeftest(nb54denelev, vcov = vcovHC(nb54denelev))
summary(nb54denelev)
#logLik(nb1propn)
overdisp_fun(nb54denelev,nulldeviance = nb54$null.deviance)
#show(nb54denelevrobustse)



```


